# -*- coding: utf-8 -*-
"""GAN_sketch.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/111bt5pqp9L-3FMA7B-Er1cQNKsPmDqxa
"""

import torch
import torch.nn as nn
from google.colab import drive
import os
drive.mount('/content/drive')

import numpy as np
data = np.load('/content/drive/My Drive/project1/dataset_strokes.npy', allow_pickle=True)

strokes[0][1]

from sklearn.metrics import mean_squared_error, mean_absolute_error
import matplotlib.pyplot as plt
"""

# Prepare the test data
test_all_strokes = []
for i in range(len(test_strokes)):
    test_all_strokes.append(test_strokes[i]['coords'])

test_inputs, test_targets = generate_training_data(test_all_strokes)
"""
# Ensure data is converted to numpy arrays
test_inputs = inputs[200:250]
test_targets = targets[200:250]

# Predict on test data
predictions = model.predict(test_inputs)

# Evaluate the predictions
mse = mean_squared_error(test_targets.reshape(-1, 2), predictions.reshape(-1, 2))
mae = mean_absolute_error(test_targets.reshape(-1, 2), predictions.reshape(-1, 2))

print(f"Mean Squared Error (MSE): {mse:.4f}")
print(f"Mean Absolute Error (MAE): {mae:.4f}")

# Visualize a few predictions
def plot_strokes(input_stroke, target_stroke, predicted_stroke, idx):
    plt.figure(figsize=(10, 5))

    # Input stroke
    plt.subplot(1, 3, 1)
    plt.plot(input_stroke[:, 0], input_stroke[:, 1], marker='o', label='Messy')
    plt.title(f"Input (Messy) - Sample {idx}")
    plt.xlabel("X")
    plt.ylabel("Y")
    plt.legend()

    # Target stroke
    plt.subplot(1, 3, 2)
    plt.plot(target_stroke[:, 0], target_stroke[:, 1], marker='o', label='Clean')
    plt.title(f"Ground Truth (Clean) - Sample {idx}")
    plt.xlabel("X")
    plt.ylabel("Y")
    plt.legend()

    # Predicted stroke
    plt.subplot(1, 3, 3)
    plt.plot(predicted_stroke[:, 0], predicted_stroke[:, 1], marker='o', label='Predicted')
    plt.title(f"Predicted Stroke - Sample {idx}")
    plt.xlabel("X")
    plt.ylabel("Y")
    plt.legend()

    plt.tight_layout()
    plt.show()

# Visualize a few samples
for i in range(3):  # Visualize 3 test samples
    idx = np.random.randint(len(test_inputs))  # Random test sample
    plot_strokes(test_inputs[idx], test_targets[idx], predictions[idx], idx)

def plot_whole_sketch(input_sketch, target_sketch, predicted_sketch, idx):
    plt.figure(figsize=(15, 5))

    # Input sketch
    plt.subplot(1, 3, 1)
    for stroke in input_sketch:
        plt.plot(stroke[:, 0], stroke[:, 1], marker='o', label='Stroke')
    plt.title(f"Input (Messy) - Sketch {idx}")
    plt.xlabel("X")
    plt.ylabel("Y")
    plt.legend()

    # Target sketch
    plt.subplot(1, 3, 2)
    for stroke in target_sketch:
        plt.plot(stroke[:, 0], stroke[:, 1], marker='o', label='Stroke')
    plt.title(f"Ground Truth (Clean) - Sketch {idx}")
    plt.xlabel("X")
    plt.ylabel("Y")
    plt.legend()

    # Predicted sketch
    plt.subplot(1, 3, 3)
    for stroke in predicted_sketch:
        plt.plot(stroke[:, 0], stroke[:, 1], marker='o', label='Stroke')
    plt.title(f"Predicted Sketch - Sketch {idx}")
    plt.xlabel("X")
    plt.ylabel("Y")
    plt.legend()

    plt.tight_layout()
    plt.show()

# Prepare data for visualization (convert sketches back to strokes)
def get_strokes_from_padded(padded_sketches):
    sketches = []
    for sketch in padded_sketches:
        strokes = []
        current_stroke = []
        for point in sketch:
            if np.all(point == 0):  # Padding indicator
                if current_stroke:
                    strokes.append(np.array(current_stroke))
                    current_stroke = []
            else:
                current_stroke.append(point)
        if current_stroke:  # Add last stroke if exists
            strokes.append(np.array(current_stroke))
        sketches.append(strokes)
    return sketches

# Convert inputs, targets, and predictions to strokes
test_input_sketches = get_strokes_from_padded(test_inputs)
test_target_sketches = get_strokes_from_padded(test_targets)
predicted_sketches = get_strokes_from_padded(predictions)

# Visualize a few test sketches
for i in range(3):  # Visualize 3 test sketches
    idx = np.random.randint(len(test_input_sketches))  # Random sketch
    plot_whole_sketch(test_input_sketches[idx], test_target_sketches[idx], predicted_sketches[idx], idx)

"""# **LSTM**"""

!pip install rdp
import torch
import torch.nn as nn
from google.colab import drive
import os
import numpy as np
from rdp import rdp
import tensorflow as tf
from tensorflow.keras import layers, models
from tensorflow.keras.preprocessing.sequence import pad_sequences
import matplotlib.pyplot as plt

# Mount Google Drive
drive.mount('/content/drive')

# Normalize Coordinates
def normalize_coords(stroke):
    """Normalize coordinates to a 0-1 range based on min/max values."""
    min_x = min([point[0] for point in stroke])
    max_x = max([point[0] for point in stroke])
    min_y = min([point[1] for point in stroke])
    max_y = max([point[1] for point in stroke])

    normalized = [[(point[0] - min_x) / (max_x - min_x + 1e-8),
                   (point[1] - min_y) / (max_y - min_y + 1e-8)] for point in stroke]
    return normalized, (min_x, max_x, min_y, max_y)

# Denormalize Coordinates
def denormalize_coords(normalized_stroke, ranges):
    """Denormalize coordinates back to their original range."""
    min_x, max_x, min_y, max_y = ranges
    denormalized = [[point[0] * (max_x - min_x) + min_x,
                     point[1] * (max_y - min_y) + min_y] for point in normalized_stroke]
    return denormalized

# Smooth Stroke
def smooth_stroke(stroke_coords, window_size=5):
    smoothed = []
    for i in range(len(stroke_coords)):
        start = max(0, i - window_size)
        end = min(len(stroke_coords), i + window_size)
        avg_x = np.mean([p[0] for p in stroke_coords[start:end]])
        avg_y = np.mean([p[1] for p in stroke_coords[start:end]])
        smoothed.append((avg_x, avg_y))
    return smoothed

# Create Clean Stroke
def create_clean_stroke(messy, epsilon=1.0, window_size=5):
    """Simulate a cleaner version of a messy stroke (e.g., smoothing)."""
    clean = []
    for i in range(len(messy)):
        window_start = max(0, i - window_size // 2)
        window_end = min(len(messy), i + window_size // 2 + 1)
        avg_x = sum(messy[j][0] for j in range(window_start, window_end)) / (window_end - window_start)
        avg_y = sum(messy[j][1] for j in range(window_start, window_end)) / (window_end - window_start)
        clean.append([avg_x, avg_y])
    return clean


# Generate Training Data
def generate_training_data(strokes, epsilon=1.0, window_size=5):
    inputs = []
    targets = []
    original_ranges = []  # To store normalization ranges for each stroke
    number_of_strokes_per_sketch = []

    for sketch in strokes:  # Iterate over all sketches
        num = 0
        for stroke in sketch:  # Iterate over all strokes within a sketch
            num += 1
            normalized_stroke, ranges = normalize_coords(stroke)  # Normalize messy stroke
            if len(normalized_stroke) < 2:  # Skip strokes with fewer than 2 points
                continue

            clean_stroke = create_clean_stroke(normalized_stroke, epsilon=epsilon, window_size=window_size)
            original_ranges.append(ranges)  # Store ranges for denormalization

            # Ensure lengths match by trimming or padding
            if len(clean_stroke) < len(normalized_stroke):
                clean_stroke.extend([clean_stroke[-1]] * (len(normalized_stroke) - len(clean_stroke)))  # Pad
            elif len(clean_stroke) > len(normalized_stroke):
                clean_stroke = clean_stroke[:len(normalized_stroke)]  # Trim

            inputs.append(np.array(normalized_stroke))
            targets.append(np.array(clean_stroke))
        number_of_strokes_per_sketch.append(num)
    # Pad sequences to make them uniform
    padded_inputs = pad_sequences(inputs, dtype='float32', padding='post')
    padded_targets = pad_sequences(targets, dtype='float32', padding='post')

    return padded_inputs, padded_targets, original_ranges, number_of_strokes_per_sketch

# Load dataset
data = np.load('/content/drive/My Drive/project1/dataset_strokes.npy', allow_pickle=True)
strokes = data[:1000, 12]  # Training strokes
test_strokes = data[1000:, 12]  # Test strokes
test_labels = data[1000:, 8]  # Test labels

# Prepare Data
all_strokes = [sketch['coords'] for sketch in strokes]  # Extract all sketch coordinates
inputs, targets, original_ranges, number_of_strokes_per_sketch = generate_training_data(all_strokes)

print("Inputs shape:", inputs.shape)
print("Targets shape:", targets.shape)

# Model Definition
model = models.Sequential([
    layers.Input(shape=(None, 2)),  # Variable-length input, 2 for (x, y)
    layers.Bidirectional(layers.LSTM(128, return_sequences=True)),  # Encoder
    layers.TimeDistributed(layers.Dense(64, activation='relu')),   # Hidden Layer
    layers.TimeDistributed(layers.Dense(2))  # Output layer: (x, y)
])

model.compile(optimizer='adam', loss='mse', metrics=['mae'])
model.summary()
from tensorflow.keras.callbacks import EarlyStopping

# Define the EarlyStopping callback
early_stopping = EarlyStopping(
    monitor='val_loss',       # Monitor the validation loss
    patience=5,               # Number of epochs with no improvement after which training will stop
    restore_best_weights=True # Restore the weights of the best epoch
)
# Training
model.fit(inputs[:300], targets[:300], batch_size=4, epochs=50, validation_split=0.2, callbacks=[early_stopping])


# Save the model
model.save('/content/drive/My Drive/project1/datasets/strokes/my_model.keras')

# Load the saved model
modeL = tf.keras.models.load_model('/content/drive/My Drive/project1/datasets/strokes/my_model.keras')

"""
TO SAVE A MODEL
# Save the model with the correct extension
model.save('/content/drive/My Drive/project1/datasets/strokes/my_model.keras')


TO LOAD A MODEL
# Load the saved model
modeL = tf.keras.models.load_model('/content/drive/My Drive/project1/datasets/strokes/my_model.keras')


"""

# Prepare Data
all_strokes_test = [sketch['coords'] for sketch in test_strokes]  # Extract all sketch coordinates

inputs, targets, original_ranges, number_of_strokes_per_sketch = generate_training_data(all_strokes_test)

test_inputs = inputs
test_targets = targets
# Get predictions from the model
predictions = modeL.predict(test_inputs)

def compare_sketches_l2(sketch1, sketch2, method='sum'):
    """
    Compare the L2 norms of two sketches using different methods.

    :param sketch1: List of strokes for Sketch 1, each stroke is [(x1, y1), (x2, y2), ...]
    :param sketch2: List of strokes for Sketch 2, same format as sketch1
    :param method: Comparison method ('sum', 'average', 'max', 'cosine')
    :return: Comparison result
    """
    def calculate_l2_norm(stroke):
        return np.sqrt(np.sum(np.array(stroke)[:, 0]**2 + np.array(stroke)[:, 1]**2))

    # Compute stroke-level L2 norms
    norms_sketch1 = [calculate_l2_norm(stroke) for stroke in sketch1]
    norms_sketch2 = [calculate_l2_norm(stroke) for stroke in sketch2]

    if method == 'sum':
        # Sum of L2 norms
        return (sum(norms_sketch1) - sum(norms_sketch2))
    elif method == 'average':
        # Average of L2 norms
        avg1 = sum(norms_sketch1) / len(norms_sketch1)
        avg2 = sum(norms_sketch2) / len(norms_sketch2)
        return (avg1 - avg2)
    elif method == 'max':
        # Maximum L2 norm
        max1 = max(norms_sketch1)
        max2 = max(norms_sketch2)
        return (max1 - max2)
    elif method == 'cosine':
        # Cosine similarity
        norms1 = np.array(norms_sketch1)
        norms2 = np.array(norms_sketch2)
        dot_product = np.dot(norms1, norms2)
        magnitude1 = np.linalg.norm(norms1)
        magnitude2 = np.linalg.norm(norms2)
        return dot_product / (magnitude1 * magnitude2 + 1e-8)
    else:
        raise ValueError("Invalid method. Choose 'sum', 'average', 'max', or 'cosine'.")

import matplotlib.pyplot as plt
import numpy as np

def denormalize_coords(normalized_strokes, ranges):
    """Denormalize multiple strokes back to their original range."""
    denormalized_strokes = []
    for stroke, range_tuple in zip(normalized_strokes, ranges):
        denormalized_stroke = []
        for point in stroke:

            min_x, max_x, min_y, max_y = range_tuple
            denormalized_x = point[0] * (max_x - min_x) + min_x
            denormalized_y = point[1] * (max_y - min_y) + min_y
            denormalized_stroke.append([denormalized_x, denormalized_y])
        denormalized_strokes.append(denormalized_stroke)
    return denormalized_strokes



def plot_two_sketches_side_by_side(denormalized_strokes, all_strokes, label, title2="Predicted Sketch"):
    """
    Plots two sketches side-by-side for comparison: denormalized and original sketches.

    Args:
        denormalized_strokes: List of strokes for the denormalized sketch.
        all_strokes: List of strokes for the original sketch.
        title1: Title for the first plot.
        title2: Title for the second plot.
    """
    fig, axes = plt.subplots(1, 2, figsize=(8, 4))

    # Plot original strokes
    for stroke in all_strokes:
        x_coords = np.array(stroke)[:, 0]
        y_coords = np.array(stroke)[:, 1]
        axes[0].plot(x_coords, y_coords, marker="o", linestyle="-", markersize=2)
    axes[0].set_title(f"Original Sketch {label}")
    axes[0].set_xlabel("X")
    axes[0].set_ylabel("Y")
    axes[0].axis("off")  # Turn off axes for the first subplot

    # Plot denormalized strokes
    for stroke in denormalized_strokes:
        x_coords = np.array(stroke)[:, 0]
        y_coords = np.array(stroke)[:, 1]
        axes[1].plot(x_coords, y_coords, marker="o", linestyle="-", markersize=2)
    axes[1].set_title(title2)
    axes[1].set_xlabel("X")
    axes[1].set_ylabel("Y")
    axes[1].axis("off")  # Turn off axes for the first subplot

    plt.tight_layout()
    plt.show()


# Remove padding from a single stroke
def remove_padding(stroke):
    """Remove padding points (zeros) from a stroke."""
    return [point for point in stroke if not np.all(point == 0)]  # Keep points that are not [0, 0]


def remove_padding_from_all_strokes(strokes):
    """Remove padding from all strokes in a sketch."""
    cleaned_strokes = []
    for stroke in strokes:
        cleaned_stroke = remove_padding(stroke)
        if len(cleaned_stroke) > 1:  # Keep only valid strokes with more than one point
            cleaned_strokes.append(cleaned_stroke)
    return cleaned_strokes


# Plot the cleaned sketch
def plot_cleaned_sketch(cleaned_strokes, title="Sketch Without Padding"):
    plt.figure(figsize=(6, 6))
    for stroke in cleaned_strokes:
        x_coords = np.array(stroke)[:, 0]
        y_coords = np.array(stroke)[:, 1]
        plt.plot(x_coords, y_coords, marker="o", linestyle="-", markersize=2)
    plt.title(title)
    plt.xlabel("X")
    plt.ylabel("Y")
    plt.show()

all_strokes_test_strokes = [stroke for sketch in test_strokes for stroke in sketch['coords']]

original_lengths = [len(stroke) for stroke in all_strokes_test_strokes]  # Track original lengths


# Remove padding from predictions using original lengths
cleaned_predictions = []
for i, pred in enumerate(predictions):
    cleaned_predictions.append(pred[:original_lengths[i]])

# Calculate cumulative sum to get start and end indices for each sketch
cumulative_sum = np.cumsum([0] + number_of_strokes_per_sketch)  # Add 0 at the beginning for indexing

# Initialize an empty list to store strokes grouped by sketch
sketches = []

# Loop through each sketch and collect its strokes
for i in range(len(number_of_strokes_per_sketch)):
    start_idx = cumulative_sum[i]
    end_idx = cumulative_sum[i + 1]
    no_pad_strokes = remove_padding_from_all_strokes(cleaned_predictions[start_idx:end_idx])
    denormalized_strokes = denormalize_coords(no_pad_strokes,original_ranges[start_idx:end_idx])
    sketches.append(denormalized_strokes)  # Collect strokes for this sketch

for i, sketch in enumerate(sketches):
    if(i < 25):
        plot_two_sketches_side_by_side(sketch, all_strokes_test[i], test_labels[i])
    i+=1

compare_sketches_l2(sketches[18], sketches[23])

def calculate_l2_norm(stroke):
    """
    Calculate the L2 norm of a single stroke.
    :param stroke: A list or array of points [(x1, y1), (x2, y2), ...]
    :return: L2 norm of the stroke
    """
    stroke = np.array(stroke)  # Ensure stroke is a NumPy array
    return np.sqrt(np.sum(stroke[:, 0]**2 + stroke[:, 1]**2))

def calculate_l2_norms_for_sketch(strokes):
    """
    Calculate the L2 norms for all strokes in a sketch.
    :param strokes: List of strokes, where each stroke is a list of points [(x1, y1), (x2, y2), ...]
    :return: List of L2 norms for each stroke
    """
    return [calculate_l2_norm(stroke) for stroke in strokes]

import matplotlib.pyplot as plt
import numpy as np

# Denormalize Coordinates
def denormalize_coords(normalized_stroke, ranges):
    """Denormalize coordinates back to their original range."""
    denormalized = []
    for stroke, range_vals in zip(normalized_stroke, ranges):
        min_x, max_x, min_y, max_y = range_vals
        denormalized_stroke = [
            [point[0] * (max_x - min_x) + min_x, point[1] * (max_y - min_y) + min_y]
            for point in stroke
        ]
        denormalized.append(denormalized_stroke)
    return denormalized


def plot_two_sketches_side_by_side(denormalized_strokes, all_strokes, title1="Clean Sketch", title2="Original Sketch"):
    """
    Plots two sketches side-by-side for comparison: denormalized and original sketches.

    Args:
        denormalized_strokes: List of strokes for the denormalized sketch.
        all_strokes: List of strokes for the original sketch.
        title1: Title for the first plot.
        title2: Title for the second plot.
    """
    fig, axes = plt.subplots(1, 2, figsize=(12, 6))

    # Plot denormalized strokes
    for stroke in denormalized_strokes:
        x_coords = np.array(stroke)[:, 0]
        y_coords = np.array(stroke)[:, 1]
        axes[0].plot(x_coords, y_coords, marker="o", linestyle="-", markersize=2)
    axes[0].set_title(title1)
    axes[0].set_xlabel("X")
    axes[0].set_ylabel("Y")

    # Plot original strokes
    for stroke in all_strokes:
        x_coords = stroke[:, 0]
        y_coords = stroke[:, 1]
        axes[1].plot(x_coords, y_coords, marker="o", linestyle="-", markersize=2)
    axes[1].set_title(title2)
    axes[1].set_xlabel("X")
    axes[1].set_ylabel("Y")

    plt.tight_layout()
    plt.show()


def remove_padding(stroke):
    """Remove padding from a stroke."""
    # Assuming padding is either [0, 0] or repeating the last valid point
    return [point for point in stroke if not (point == [0, 0] or np.allclose(point, stroke[-1]))]

def remove_padding_from_all_strokes(strokes):
    """Remove padding from all strokes in a sketch."""
    cleaned_strokes = []
    for stroke in strokes:
        cleaned_stroke = remove_padding(stroke)
        if len(cleaned_stroke) > 1:  # Keep only valid strokes with more than one point
            cleaned_strokes.append(cleaned_stroke)
    return cleaned_strokes


# Plot the cleaned sketch
def plot_cleaned_sketch(cleaned_strokes, title="Sketch Without Padding"):
    plt.figure(figsize=(6, 6))
    for stroke in cleaned_strokes:
        x_coords = np.array(stroke)[:, 0]
        y_coords = np.array(stroke)[:, 1]
        plt.plot(x_coords, y_coords, marker="o", linestyle="-", markersize=2)
    plt.title(title)
    plt.xlabel("X")
    plt.ylabel("Y")
    plt.show()

print("Inputs shape:", inputs.shape)
print("Targets shape:", targets.shape)

# Model Definition
model = models.Sequential([
    layers.Input(shape=(None, 2)),  # Variable-length input, 2 for (x, y)
    layers.Bidirectional(layers.LSTM(128, return_sequences=True)),  # Encoder
    layers.TimeDistributed(layers.Dense(64, activation='relu')),   # Hidden Layer
    layers.TimeDistributed(layers.Dense(2))  # Output layer: (x, y)
])

model.compile(optimizer='adam', loss='mse', metrics=['mae'])
model.summary()
from tensorflow.keras.callbacks import EarlyStopping

# Define the EarlyStopping callback
early_stopping = EarlyStopping(
    monitor='val_loss',       # Monitor the validation loss
    patience=5,               # Number of epochs with no improvement after which training will stop
    restore_best_weights=True # Restore the weights of the best epoch
)
# Training
model.fit(targets[:300], inputs[:300], batch_size=4, epochs=50, validation_split=0.2, callbacks=[early_stopping])


# Save the model
model.save('/content/drive/My Drive/project1/datasets/strokes/my_model_reverse.keras')

# Prepare Data
all_strokes_test = [sketch['coords'] for sketch in test_strokes]  # Extract all sketch coordinates

inputs, targets, original_ranges, number_of_strokes_per_sketch = generate_training_data(all_strokes_test)

test_inputs = inputs
test_targets = targets
# Get predictions from the model
predictions = model.predict(test_inputs)

all_strokes_test_strokes = [stroke for sketch in test_strokes for stroke in sketch['coords']]

original_lengths = [len(stroke) for stroke in all_strokes_test_strokes]  # Track original lengths


# Remove padding from predictions using original lengths
cleaned_predictions = []
for i, pred in enumerate(predictions):
    cleaned_predictions.append(pred[:original_lengths[i]])

# Calculate cumulative sum to get start and end indices for each sketch
cumulative_sum = np.cumsum([0] + number_of_strokes_per_sketch)  # Add 0 at the beginning for indexing

# Initialize an empty list to store strokes grouped by sketch
sketches = []

# Loop through each sketch and collect its strokes
for i in range(len(number_of_strokes_per_sketch)):
    start_idx = cumulative_sum[i]
    end_idx = cumulative_sum[i + 1]
    no_pad_strokes = remove_padding_from_all_strokes(cleaned_predictions[start_idx:end_idx])
    denormalized_strokes = denormalize_coords(no_pad_strokes,original_ranges[start_idx:end_idx])
    sketches.append(denormalized_strokes)  # Collect strokes for this sketch

for i, sketch in enumerate(sketches):
    if(i < 25):
        plot_two_sketches_side_by_side(sketch, all_strokes_test[i], test_labels[i])
    i+=1